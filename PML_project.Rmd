---
title: "PML_project"
author: "arnela82"
date: "7/20/2020"
output:
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The main goal of the Project is to quantify how well an individual perform for a particular activity. This will be accomplished by training a prediction model on the accelerometer data. The algorithm that we will be using for this exercise will be a random forest classifier.

We will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. People were asked to perform barbell lifts correctly and incorrectly in 5 different ways. 

The training data and the test data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.

##Load libraries
```{r cache=TRUE}
library(randomForest)
library(caret)
library(rpart)
library(rpart.plot)

```

##Download and load data
Now we will download and load training and test set
```{r}
train_url<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainFile<-"./coursera/pml-training.csv"
testFile<-"./coursera/pml-testing.csv"

```

##Read the data
After downloading the data from data source, we will read two csv files into data frame

```{r cache=TRUE}
trainR<- read.csv("C:/Users/Arnela/Desktop/coursera/pml-training.csv")
testR<-read.csv("C:/Users/Arnela/Desktop/coursera/pml-testing.csv")
dim(trainR)
dim(testR)
```
We can see that the training data set contains 19662 observations and 160 variables, while test data set contains 20 observations and 160 variables.The "classe" variable in the training set is the outcome to predict. 

##Clean the data
Now, we will clean the data and get rid of observations with missing values as well as some meaningless variables.

```{r, cache=TRUE}
sum(complete.cases(trainR))
```
We will remove columns that contain NA
```{r, cache=TRUE}
trainR<-trainR[,colSums(is.na(trainR))==0]
testR<-testR[,colSums(is.na(testR))==0]
```


Next, we get rid of some columns that do not contribute much to the accelerometer measurements.

```{r, cache=TRUE}
classe<-trainR$classe
trainRemove<-grepl("^X|timespamp|window", names(trainR))
trainR<-trainR[,!trainRemove]
trainClean<-trainR[,sapply(trainR,is.numeric)]
trainClean$classe<-classe
testRemove<-grepl("^X|timestamp|window", names(testR))
testR<-testR[,!testRemove]
testClean<-testR[,sapply(testR, is.numeric)]
```

##Create train and test data 70 - 30
We can split our trining set into a training data set ( 70% ) and test/validation set ( 30% ). We will use the validation data set to conduct cross validation in future steps.  
```{r, cache = TRUE}
set.seed(22519)
inTrain<-createDataPartition(trainClean$classe, p=.7, list=FALSE)
trainData<- trainClean[inTrain, ]
testData<-trainClean[-inTrain, ]
```

##Data modeling
I used the *random-forest* technique to generate a predictive model. In sum, 10 models were trained. I played around with the parameters passed to `trControl` and specified different models with bootstrapping (`method = "boot"`) and cross-validation (`method = "cv"`).

It took more than one day to train all models. Afterwards I tested their performance on the cross-validation dataset. It turned out that all models showed a good performance (because their accuracy was above 99%) though their training times were quite different.

Due to the similar performance, I will present the model with the shortest training time.


```{r, cache=TRUE}
library(randomForest)
trControl<- trainControl(method = "cv", number = 2)
modFit<-train(classe ~ ., data= trainData, method="rf", trControl=trControl)
```

####Evaluate the model ( out - of -sample error )

First we'll make predictions on outcome in cross-validation dataset.
```{r predict, cache=TRUE}
pred<- predict(modFit, newdata=testData)

```

Second, the function 'confusionMatrix' is used to calculate the accuracy of the prediction.

```{r accuracy}
cM<-confusionMatrix(pred, reference = testData$classe)
acc<-cM$overall["Accuracy"]
acc
```
The accuracy of the prediction is `r paste0(round(acc * 100, 2), "%")`. Hence, the *out-of-sample error* is `r paste0(round(100 - acc * 100, 2), "%")`.




